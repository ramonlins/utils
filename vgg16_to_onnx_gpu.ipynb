{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c113a3a1",
   "metadata": {},
   "source": [
    "# Accelerating and scaling inference with ONNX on GPU\n",
    "## 01 - Getting started\n",
    "#### By Ramon Lins\n",
    "------------------\n",
    "\n",
    "**Table of contents**\n",
    "* [Introduction](#introduction)\n",
    "* [Setup](#setup)\n",
    "* [Tutorial](#tutorial)\n",
    "* [Visualization](#zetane)\n",
    "* [Optional](#option)\n",
    "\n",
    "Reference:\n",
    "\n",
    "- Tutorial\n",
    "    > https://pytorch.org/tutorials/advanced/super_resolution_with_ort.html(cpu tutorial)\n",
    "    \n",
    "    > https://pytorch.org/docs/master/onnx.html\n",
    "\n",
    "- Setup\n",
    "    > https://onnxruntime.ai/\n",
    "\n",
    "    > https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal (cuda 10.1)\n",
    "\n",
    "    > https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.2_20191118/cudnn-10.2-linux-x64-v7.6.5.32.tgz (cudnn 7.6.5)\n",
    "\n",
    "    > https://docs.nvidia.com/cuda/cuda-installation-guide-linux/\n",
    "\n",
    "- ONNX\n",
    "    > https://onnxruntime.ai/docs/tutorials/export-pytorch-model.html\n",
    "    \n",
    "    > https://pytorch.org/docs/master/onnx.html\n",
    "\n",
    "- ONNXRuntime\n",
    "    > https://onnxruntime.ai/docs/tutorials/\n",
    "    \n",
    "    > https://github.com/microsoft/onnxruntime\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/tutorials/accelerate-pytorch/pytorch.html\n",
    "\n",
    "    > https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_GPU.ipynb (gpu tutorial)\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html(version compatibility)\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/build/eps.html#cuda\n",
    "    \n",
    "- Visualization\n",
    "    > https://github.com/onnx/tutorials/blob/main/tutorials/VisualizingAModel.md\n",
    "\n",
    "- Comparison\n",
    "    > https://github.com/onnx/tutorials/blob/main/tutorials/CorrectnessVerificationAndPerformanceComparison.ipynb\n",
    "\n",
    "- Optional\n",
    "    > https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/converter_scripts/float32_float16_onnx.ipynb\n",
    "    \n",
    "    > https://github.com/onnx/onnx-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aecf0f",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762887f",
   "metadata": {},
   "source": [
    "<a id=\"introduction\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd59257",
   "metadata": {},
   "source": [
    "ONNX is an open source project designed to accelerate machine learning across a wide variety of \n",
    "frameworks, operating systems, and hardware platforms.\n",
    "\n",
    "The main objective of this task is to use the ONNX engine to optimize the patch-based density model,\n",
    "a vgg-16 customized network, to reducing latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e6c1c",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12592b88",
   "metadata": {},
   "source": [
    "Create a environment.yml with:\n",
    "```\n",
    "name: onnx_gpu\n",
    "channels:\n",
    "  - pytorch-lts\n",
    "dependencies:\n",
    "  - python=3.7.*\n",
    "  - pytorch=1.8.2\n",
    "  - torchvision=0.9.2\n",
    "  - cudatoolkit=10.1\n",
    "  - pip\n",
    "  - pip:\n",
    "      - onnx\n",
    "      - onnxruntime-gpu==1.4\n",
    "```\n",
    "run in terminal\n",
    "\n",
    "```\n",
    "conda env create\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18b7cd",
   "metadata": {},
   "source": [
    "Prerequistes to run the jupyter notebook:\n",
    "```bash\n",
    "conda activate onnx_gpu\n",
    "conda install -c anaconda ipykernel\n",
    "conda install -c conda-forge ipywidgets\n",
    "python -m ipykernel install --user --name=onnx_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257940d8",
   "metadata": {},
   "source": [
    "<a id=\"tutorial\"></a>\n",
    "### Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a518b3b",
   "metadata": {},
   "source": [
    "Pytorch use build-in cuda and cudnn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafc4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.8.2\n",
      "cuda version: 10.1\n",
      "cudnn version: 7603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.onnx\n",
    "\n",
    "print(\"pytorch version:\", torch.__version__)\n",
    "print(\"cuda version:\" , torch.version.cuda)\n",
    "print(\"cudnn version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32777209",
   "metadata": {},
   "source": [
    "To handle cuda 10.1 and cudnn 7603 of pytorch, the most close version of onnxruntime-gpu is 1.4 with\n",
    "cuda 10.1 and cudnn 765. (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a81f39",
   "metadata": {},
   "source": [
    "To run onnx runtime, cuda and cudnn version should be installed from source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d286c1a",
   "metadata": {},
   "source": [
    "To install ***cuda*** (10.1) from source follow the instructions:\n",
    "\n",
    "```bash\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
    "\n",
    "sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "\n",
    "wget https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
    "\n",
    "sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
    "\n",
    "sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\n",
    "\n",
    "sudo apt-get update\n",
    "\n",
    "sudo apt-get -y install cuda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28557a4e",
   "metadata": {},
   "source": [
    "To install ***cudnn*** (7.6.5) from source follow the instructions:\n",
    "\n",
    "```bash\n",
    "wget https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.2_20191118/cudnn-10.2-linux-x64-v7.6.5.32.tgz\n",
    "\n",
    "tar -zxf cudnn-10.2-linux-x64-v7.6.5.32.tgz\n",
    "\n",
    "cd cuda\n",
    "sudo cp -P lib64/* /usr/local/cuda/lib64/\n",
    "sudo cp -P include/* /usr/local/cuda/include/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd3f74",
   "metadata": {},
   "source": [
    "Perhaps environment paths are not set correctly, so after Install CUDA and cuDNN:\n",
    "- The path to the CUDA installation must be provided via the `CUDA_PATH` environment variable\n",
    "- The path to the cuDNN installation (include the cuda folder in the path) must be provided via the `cuDNN_PATH` environment variable. The cuDNN path should contain bin, include and lib directories.\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n",
    "\n",
    "export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n",
    "\n",
    "export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbc6d9",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd3b74",
   "metadata": {},
   "source": [
    "Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630f0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from models.estimator import Estimator\n",
    "\n",
    "img_res = (512, 512)\n",
    "model_path = \"/home/ramon/Git/adroit/vision_foliage_density/weights/foliage_density_v3/density_model_reg.pth\"\n",
    "\n",
    "predictor = Estimator((512, 512), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa96263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 10\n",
    "total_samples = 100//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10d1a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input data\n",
    "torch_inputs = torch.randn(batch_size, 3, 512, 512, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9aca49c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 3.95 GiB total capacity; 2.31 GiB already allocated; 222.00 MiB free; 2.58 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13521/4108201398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtotal_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/utils/models/estimator.py\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/utils/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/utils/models/vgg_reg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0msize_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mres_feat0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mres_feat0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_feat0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/utils/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[1;32m   3552\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0malign_corners\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_bilinear2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"trilinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0malign_corners\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 3.95 GiB total capacity; 2.31 GiB already allocated; 222.00 MiB free; 2.58 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# inference torch\n",
    "total_start = time.time()\n",
    "for i in range(total_samples):\n",
    "    torch_input = torch_inputs.to(device)\n",
    "    predictor.estimate(torch_input)\n",
    "total_end = time.time()\n",
    "\n",
    "print(f\"Pytorch total inference time = {total_end - total_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddde7b6",
   "metadata": {},
   "source": [
    "Loading model to gpu seems to be the main problem of latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdff73",
   "metadata": {},
   "source": [
    "Export pytorch to onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dd2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx version: 1.12.0\n",
      "onnxruntime version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort.set_default_logger_severity(0)\n",
    "\n",
    "print(\"onnx version:\", onnx.__version__)\n",
    "print(\"onnxruntime version:\", ort.__version__)\n",
    "\n",
    "onnx_version = onnx.__version__.split('.')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d44f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'VGGReg' object has no attribute 'graph'\n"
     ]
    }
   ],
   "source": [
    "# input example necessary to export onnx model\n",
    "torch_input = torch.randn(batch_size, 3, 512, 512, requires_grad=True).to(device)\n",
    "\n",
    "model = Estimator((512, 512), model_path).model\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, # model being run\n",
    "    torch_input, # model input (or a tuple for multiple inputs)\n",
    "    \"density.onnx\", # where to save the model (can be a file or file-like object)\n",
    "    opset_version=12,\n",
    "    export_params=True, # store the trained parameter weights inside the model file\n",
    "    input_names=['input'], # the model's input names\n",
    "    output_names=['output']) # the model's output names\n",
    "    #dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}) # variable length axes\n",
    "\n",
    "try:\n",
    "    # print human readable representation of the graph if exist\n",
    "    print(onnx.helper.printable_graph(model.graph))\n",
    "except AttributeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e350a1e",
   "metadata": {},
   "source": [
    "Before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX’s API. \n",
    "\n",
    "1. First, onnx.load(\"superres.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). \n",
    "2. Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba4cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export is valid.\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"density.onnx\")\n",
    "# check consistency of the model, if model is larger than 2GB, model should be checked with \n",
    "# path instead of model itself\n",
    "print(\"ONNX export is valid.\") if onnx.checker.check_model(onnx_model) == None else print(\"ONNX export is invalid.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e7510",
   "metadata": {},
   "source": [
    "Compute inference output using onnx runtime.\n",
    "\n",
    "To run the model, it is necessary to create an inference session. Once it is created, the model is evaluated using `run()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae2b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX and PyTorch results match!\n"
     ]
    }
   ],
   "source": [
    "ort.set_default_logger_severity(2)\n",
    "\n",
    "# create a onnx runtime session\n",
    "session = ort.InferenceSession(\"density.onnx\")\n",
    "\n",
    "# NOTE: This can be a bottleneck for gpu devices, since the tensor is transferred to cpu to\n",
    "# convert to numpy array. Then, the numpy array is transferred to gpu .\n",
    "\n",
    "# turn tensor into numpy array \n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# inference onnx\n",
    "onnx_output = session.run(None, {\"input\": to_numpy(torch_input)},)\n",
    "\n",
    "# inference pytorch\n",
    "torch_output = predictor.estimate(torch_input)\n",
    "\n",
    "# compare onnx X with pytorch results\n",
    "try:\n",
    "    np.testing.assert_allclose(to_numpy(torch_output), onnx_output[0], rtol=1e-03, atol=1e-05)\n",
    "    print(\"ONNX and PyTorch results match!\")\n",
    "except AssertionError as error:\n",
    "    print(\"ONNX and PyTorch results do not match!\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16403c87",
   "metadata": {},
   "source": [
    "Test latency inference output with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX inference time = 168.41169810295105\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(0, total_samples, batch_size):\n",
    "    onnx_input = {\"input\": to_numpy(torch_inputs[i:i+batch_size])}\n",
    "    onnx_output = session.run(None, onnx_input)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"ONNX inference time = {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('utils')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e890fa95ed0ec9c8fb286e04549f6077243facc59f1dcdc56b3adff9cce08d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
