{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c113a3a1",
   "metadata": {},
   "source": [
    "# Accelerating and scaling inference with ONNX in CPU\n",
    "## 01 - Getting started\n",
    "#### By Ramon Lins\n",
    "------------------\n",
    "\n",
    "**Table of contents**\n",
    "* [Introduction](#introduction)\n",
    "* [Setup](#setup)\n",
    "* [Tutorial](#tutorial)\n",
    "* [Visualization](#zetane)\n",
    "* [Optional](#option)\n",
    "\n",
    "Reference:\n",
    "\n",
    "- Tutorial\n",
    "    > https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n",
    "    \n",
    "    > https://pytorch.org/docs/master/onnx.html\n",
    "\n",
    "- Setup\n",
    "    > https://onnxruntime.ai/\n",
    "\n",
    "- ONNX\n",
    "    > https://onnxruntime.ai/docs/tutorials/export-pytorch-model.html\n",
    "    \n",
    "    > https://pytorch.org/docs/master/onnx.html\n",
    "\n",
    "- ONNXRuntime\n",
    "    > https://onnxruntime.ai/docs/tutorials/\n",
    "    \n",
    "    > https://github.com/microsoft/onnxruntime\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/tutorials/accelerate-pytorch/pytorch.html\n",
    "\n",
    "- Visualization\n",
    "    > https://github.com/onnx/tutorials/blob/main/tutorials/VisualizingAModel.md\n",
    "\n",
    "- Comparison\n",
    "    > https://github.com/onnx/tutorials/blob/main/tutorials/CorrectnessVerificationAndPerformanceComparison.ipynb\n",
    "\n",
    "- Optional\n",
    "    > https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/converter_scripts/float32_float16_onnx.ipynb\n",
    "    \n",
    "    > https://github.com/onnx/onnx-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aecf0f",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762887f",
   "metadata": {},
   "source": [
    "<a id=\"introduction\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd59257",
   "metadata": {},
   "source": [
    "ONNX is an open source project designed to accelerate machine learning across a wide variety of \n",
    "frameworks, operating systems, and hardware platforms.\n",
    "\n",
    "The main objective of this task is to use the ONNX engine to optimize the patch-based density model,\n",
    "a vgg-16 customized network, to reducing latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e6c1c",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12592b88",
   "metadata": {},
   "source": [
    "Create a environment.yml with:\n",
    "```\n",
    "name: base\n",
    "channels:\n",
    "  - pytorch-lts\n",
    "dependencies:\n",
    "  - python=3.7.*\n",
    "  - pytorch=1.8.2\n",
    "  - torchvision=0.9.2\n",
    "  - cudatoolkit=10.2\n",
    "  - pip\n",
    "  - pip:\n",
    "      - onnx==1.12.0\n",
    "      - onnxruntime==1.11.1\n",
    "```\n",
    "next run the command:\n",
    "```bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c02d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda env create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02ab12",
   "metadata": {},
   "source": [
    "if an environment already exist, install onnx direct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9cb4ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.12.0 in /home/ramon/anaconda3/envs/foliage_density/lib/python3.7/site-packages (1.12.0)\n",
      "Requirement already satisfied: onnxruntime==1.11.1 in /home/ramon/anaconda3/envs/foliage_density/lib/python3.7/site-packages (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/ramon/anaconda3/envs/foliage_density/lib/python3.7/site-packages (from onnx==1.12.0) (4.1.1)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /home/ramon/anaconda3/envs/foliage_density/lib/python3.7/site-packages (from onnx==1.12.0) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ramon/anaconda3/envs/foliage_density/lib/python3.7/site-packages (from onnx==1.12.0) (1.21.2)\n",
      "Requirement already satisfied: flatbuffers in /home/ramon/anaconda3/envs/foliage_density/lib/python3.7/site-packages (from onnxruntime==1.11.1) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install onnx==1.12.0 onnxruntime==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257940d8",
   "metadata": {},
   "source": [
    "<a id=\"tutorial\"></a>\n",
    "### Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cafc4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.init as init\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "630f0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor: int):\n",
    "        \"\"\"Super Resolution Network for increasing the resolutiono of images\n",
    "\n",
    "        Args:\n",
    "            upscale_factor (int): The factor by which the image resolution is increased\n",
    "        \"\"\"\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "        batch_size = 1 # Batch size\n",
    "        num_filters = 64 # number of filters\n",
    "        kernel_size_in = 5 # 5x5 kernel for input convolution\n",
    "        kernel_size_hl = 3 # 3x3 kernel for hidden layer convolution\n",
    "        stride = 1 # stride of the convolution\n",
    "        padding_in = 2 # padding for input convolution\n",
    "        padding_hl = 1 # padding for hidden layers\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(batch_size, num_filters, kernel_size_in, stride, padding_in)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size_hl, stride, padding_hl)\n",
    "        self.conv3 = nn.Conv2d(num_filters, num_filters//2, kernel_size_hl, stride, padding_hl)\n",
    "        self.conv4 = nn.Conv2d(num_filters//2, upscale_factor**2, kernel_size_hl, stride, padding_hl)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward operation\n",
    "\n",
    "        Args:\n",
    "            x (tensor): input image of shape (batch_size, 1, H, W)\n",
    "\n",
    "        Returns:\n",
    "            tensor: output image of shape (batch_size, 1, H*upscale_factor, W*upscale_factor)\n",
    "        \"\"\"\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        return self.pixel_shuffle(self.conv4(x))\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        #initialize weights for the network using orthogonal initialization\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "\n",
    "\n",
    "model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdff73",
   "metadata": {},
   "source": [
    "Export onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9d44f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1\n",
    "\n",
    "# pretrained model weights\n",
    "model.load_state_dict(model_zoo.load_url(model_url))\n",
    "\n",
    "# evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# input image to test onnx model\n",
    "torch_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_output = model(torch_input)\n",
    "\n",
    "# export the model to onnx\n",
    "torch.onnx.export(\n",
    "    model, # model being run\n",
    "    torch_input, # model input (or a tuple for multiple inputs)\n",
    "    \"superres.onnx\", # where to save the model (can be a file or file-like object)\n",
    "    export_params=True, # store the trained parameter weights inside the model file\n",
    "    opset_version=12, # the ONNX version to export the model to\n",
    "    do_constant_folding=True, # whether to execute constant folding\n",
    "    input_names=['input'], # the model's input names\n",
    "    output_names=['output'], # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}) # variable length axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e350a1e",
   "metadata": {},
   "source": [
    "Before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX’s API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ba4cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"superres.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e7510",
   "metadata": {},
   "source": [
    "Compute output using onnx runtime.\n",
    "\n",
    "To run the model, it is necessary create an inference session. Once it is created, the model is evaluated using `run()` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33ae2b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX and PyTorch results do not match!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"superres.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# inference\n",
    "onnx_inputs = {session.get_inputs()[0].name: to_numpy(torch_input)}\n",
    "onnx_output = session.run(None, onnx_inputs)\n",
    "\n",
    "# compare onnx X with pytorch results\n",
    "if np.testing.assert_allclose(to_numpy(torch_output), onnx_output[0], rtol=1e-03, atol=1e-05):\n",
    "    print(\"ONNX and PyTorch results match!\")\n",
    "else:\n",
    "    print(\"ONNX and PyTorch results do not match!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d7d48",
   "metadata": {},
   "source": [
    "Test using images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6bcd6773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2157, 0.1961, 0.1922,  ..., 0.5294, 0.5569, 0.5725],\n",
       "          [0.2039, 0.1922, 0.1922,  ..., 0.5333, 0.5529, 0.5686],\n",
       "          [0.2000, 0.1843, 0.1843,  ..., 0.5216, 0.5373, 0.5490],\n",
       "          ...,\n",
       "          [0.6667, 0.6745, 0.6392,  ..., 0.6902, 0.6667, 0.6078],\n",
       "          [0.6392, 0.6431, 0.6235,  ..., 0.8000, 0.7608, 0.6745],\n",
       "          [0.6392, 0.6353, 0.6510,  ..., 0.8118, 0.7686, 0.6667]]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img = Image.open(\"/home/ramon/Git/utils/img/cat.jpg\")\n",
    "\n",
    "resize= transforms.Resize([224, 224])\n",
    "img_rs = resize(img)\n",
    "\n",
    "img_ycbcr = img_rs.convert('YCbCr')\n",
    "img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_y = to_tensor(img_y)\n",
    "img_y.unsqueeze_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bb6acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "onnx_inputs = {session.get_inputs()[0].name: to_numpy(img_y)}\n",
    "onnx_output = session.run(None, onnx_inputs)\n",
    "img_out_y = onnx_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ca931d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n",
    "\n",
    "# get the output image follow post-processing step from PyTorch implementation\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")\n",
    "\n",
    "# Save the image, we will compare this with the output image from mobile device\n",
    "final_img.save(\"./out/cat.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1a2cb",
   "metadata": {},
   "source": [
    "Comparison of time running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16f26286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference torch\n",
    "torch_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_output = model(torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5be8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "onnx_inputs = {session.get_inputs()[0].name: to_numpy(torch_input)}\n",
    "onnx_output = session.run(None, onnx_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d8807",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('foliage_density')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1696ca677455f233eb8dfd00bd997f50951630100683c40b1da2b26abb1346f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
