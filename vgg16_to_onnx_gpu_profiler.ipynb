{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c113a3a1",
   "metadata": {},
   "source": [
    "# Accelerating and scaling inference with ONNX on GPU\n",
    "## 01 - Getting started\n",
    "#### By Ramon Lins\n",
    "------------------\n",
    "\n",
    "**Table of contents**\n",
    "* [Introduction](#introduction)\n",
    "* [Setup](#setup)\n",
    "* [Tutorial](#tutorial)\n",
    "* [Visualization](#zetane)\n",
    "* [Optional](#option)\n",
    "\n",
    "Reference:\n",
    "\n",
    "- Tutorial\n",
    "    > https://pytorch.org/tutorials/advanced/super_resolution_with_ort.html(cpu tutorial)\n",
    "    \n",
    "    > https://pytorch.org/docs/master/onnx.html\n",
    "\n",
    "- Setup\n",
    "    > https://onnxruntime.ai/\n",
    "\n",
    "    > https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal (cuda 10.1)\n",
    "\n",
    "    > https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.2_20191118/cudnn-10.2-linux-x64-v7.6.5.32.tgz (cudnn 7.6.5)\n",
    "\n",
    "    > https://docs.nvidia.com/cuda/cuda-installation-guide-linux/\n",
    "\n",
    "- ONNX\n",
    "    > https://onnxruntime.ai/docs/tutorials/export-pytorch-model.html\n",
    "    \n",
    "    > https://pytorch.org/docs/master/onnx.html\n",
    "\n",
    "- ONNXRuntime\n",
    "    > https://onnxruntime.ai/docs/tutorials/\n",
    "    \n",
    "    > https://github.com/microsoft/onnxruntime\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/tutorials/accelerate-pytorch/pytorch.html\n",
    "\n",
    "    > https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_GPU.ipynb (gpu tutorial)\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html(version compatibility)\n",
    "    \n",
    "    > https://onnxruntime.ai/docs/build/eps.html#cuda\n",
    "    \n",
    "- Visualization\n",
    "    > https://github.com/onnx/tutorials/blob/main/tutorials/VisualizingAModel.md\n",
    "\n",
    "- Comparison\n",
    "    > https://github.com/onnx/tutorials/blob/main/tutorials/CorrectnessVerificationAndPerformanceComparison.ipynb\n",
    "\n",
    "- Optional\n",
    "    > https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/converter_scripts/float32_float16_onnx.ipynb\n",
    "    \n",
    "    > https://github.com/onnx/onnx-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aecf0f",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762887f",
   "metadata": {},
   "source": [
    "<a id=\"introduction\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd59257",
   "metadata": {},
   "source": [
    "ONNX is an open source project designed to accelerate machine learning across a wide variety of \n",
    "frameworks, operating systems, and hardware platforms.\n",
    "\n",
    "The main objective of this task is to use the ONNX engine to optimize the patch-based density model,\n",
    "a vgg-16 customized network, to reducing latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e6c1c",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12592b88",
   "metadata": {},
   "source": [
    "Create a environment.yml with:\n",
    "```\n",
    "name: onnx_gpu\n",
    "channels:\n",
    "  - pytorch-lts\n",
    "dependencies:\n",
    "  - python=3.7.*\n",
    "  - pytorch=1.8.2\n",
    "  - torchvision=0.9.2\n",
    "  - cudatoolkit=10.1\n",
    "  - pip\n",
    "  - pip:\n",
    "      - onnx\n",
    "      - onnxruntime-gpu==1.4\n",
    "```\n",
    "run in terminal\n",
    "\n",
    "```\n",
    "conda env create\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18b7cd",
   "metadata": {},
   "source": [
    "Prerequistes to run the jupyter notebook:\n",
    "```bash\n",
    "conda activate onnx_gpu\n",
    "conda install -c anaconda ipykernel\n",
    "conda install -c conda-forge ipywidgets\n",
    "python -m ipykernel install --user --name=onnx_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257940d8",
   "metadata": {},
   "source": [
    "<a id=\"tutorial\"></a>\n",
    "### Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a518b3b",
   "metadata": {},
   "source": [
    "Pytorch use build-in cuda and cudnn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cafc4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.8.2\n",
      "cuda version: 10.1\n",
      "cudnn version: 7603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.onnx\n",
    "\n",
    "print(\"pytorch version:\", torch.__version__)\n",
    "print(\"cuda version:\" , torch.version.cuda)\n",
    "print(\"cudnn version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a81f39",
   "metadata": {},
   "source": [
    "To run onnx runtime, cuda and cudnn version should be installed from source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d286c1a",
   "metadata": {},
   "source": [
    "To install ***cuda*** (10.1) from source follow the instructions:\n",
    "\n",
    "```bash\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
    "\n",
    "sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "\n",
    "wget https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
    "\n",
    "sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
    "\n",
    "sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\n",
    "\n",
    "sudo apt-get update\n",
    "\n",
    "sudo apt-get -y install cuda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28557a4e",
   "metadata": {},
   "source": [
    "To install ***cudnn*** (7.6.5) from source follow the instructions:\n",
    "\n",
    "```bash\n",
    "wget https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.2_20191118/cudnn-10.2-linux-x64-v7.6.5.32.tgz\n",
    "\n",
    "tar -zxf cudnn-10.2-linux-x64-v7.6.5.32.tgz\n",
    "\n",
    "cd cuda\n",
    "sudo cp -P lib64/* /usr/local/cuda/lib64/\n",
    "sudo cp -P include/* /usr/local/cuda/include/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd3f74",
   "metadata": {},
   "source": [
    "Perhaps environment paths are not set correctly, so after Install CUDA and cuDNN:\n",
    "- The path to the CUDA installation must be provided via the `CUDA_PATH` environment variable\n",
    "- The path to the cuDNN installation (include the cuda folder in the path) must be provided via the `cuDNN_PATH` environment variable. The cuDNN path should contain bin, include and lib directories.\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n",
    "\n",
    "export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n",
    "\n",
    "export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbc6d9",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd3b74",
   "metadata": {},
   "source": [
    "Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "630f0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.estimator import Estimator\n",
    "\n",
    "img_res = (512, 512)\n",
    "model_path = \"/home/ramon/Git/adroit/vision_foliage_density/weights/foliage_density_v3/density_model_reg.pth\"\n",
    "\n",
    "predictor = Estimator((512, 512), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aa96263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "total_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10d1a8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3, 512, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random input data\n",
    "torch_inputs = torch.randn(total_samples, 3, 512, 512, requires_grad=False)\n",
    "torch_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aca49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference torch\n",
    "# Remeber that profiler can add delay\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as pytorch_profiler:\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        torch_input = torch_inputs[i:i+batch_size].to(device)\n",
    "        predictor.estimate(torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6bae7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "          aten::cudnn_convolution         1.54%        1.200s         1.66%        1.293s      58.760us       59.034s        74.43%       59.034s       2.683ms         22000  \n",
      "                       aten::add_         0.58%     449.405ms         0.58%     449.405ms      20.427us        7.735s         9.75%        7.735s     351.600us         22000  \n",
      "                 aten::threshold_         0.32%     252.075ms         0.32%     252.075ms      10.960us        6.446s         8.13%        6.446s     280.257us         23000  \n",
      "    aten::max_pool2d_with_indices         0.17%     133.033ms         0.22%     175.098ms      35.020us        2.911s         3.67%        2.911s     582.142us          5000  \n",
      "                      aten::addmm         0.18%     143.670ms         0.21%     166.408ms      55.469us        1.487s         1.87%        1.487s     495.566us          3000  \n",
      "        aten::upsample_bilinear2d         0.11%      86.935ms         0.16%     121.238ms      30.309us     724.740ms         0.91%     724.740ms     181.185us          4000  \n",
      "                      aten::copy_        94.83%       74.064s        94.83%       74.064s      74.064ms     587.715ms         0.74%     587.715ms     587.715us          1000  \n",
      "               aten::_convolution         0.56%     438.334ms         3.08%        2.406s     109.351us     103.356ms         0.13%       66.895s       3.041ms         22000  \n",
      "                      aten::relu_         0.45%     350.738ms         0.77%     602.813ms      26.209us     101.937ms         0.13%        6.548s     284.689us         23000  \n",
      "                     aten::conv2d         0.24%     183.595ms         3.52%        2.747s     124.883us      54.180ms         0.07%       67.000s       3.045ms         22000  \n",
      "                aten::convolution         0.20%     158.108ms         3.28%        2.564s     116.538us      51.440ms         0.06%       66.946s       3.043ms         22000  \n",
      "                    aten::reshape         0.14%     107.615ms         0.29%     225.257ms      10.239us      22.463ms         0.03%      22.463ms       1.021us         22000  \n",
      "                       aten::_cat         0.03%      21.499ms         0.04%      27.596ms      27.596us      15.277ms         0.02%      15.277ms      15.277us          1000  \n",
      "                 aten::max_pool2d         0.07%      51.862ms         0.29%     226.960ms      45.392us      10.202ms         0.01%        2.921s     584.183us          5000  \n",
      "                         aten::to         0.02%      15.651ms        94.86%       74.085s      74.085ms      10.098ms         0.01%     597.813ms     597.813us          1000  \n",
      "                    aten::sigmoid         0.03%      24.409ms         0.04%      27.671ms      27.671us       8.165ms         0.01%       8.165ms       8.165us          1000  \n",
      "                     aten::linear         0.05%      36.111ms         0.30%     230.455ms      76.818us       6.059ms         0.01%        1.493s     497.585us          3000  \n",
      "                        aten::cat         0.02%      18.623ms         0.06%      46.219ms      46.219us       4.103ms         0.01%      19.380ms      19.380us          1000  \n",
      "                      aten::slice         0.01%       7.127ms         0.01%       9.029ms       9.029us       0.000us         0.00%       0.000us       0.000us          1000  \n",
      "                 aten::as_strided         0.02%      12.365ms         0.02%      12.365ms       1.766us       0.000us         0.00%       0.000us       0.000us          7000  \n",
      "              aten::empty_strided         0.01%       5.219ms         0.01%       5.219ms       5.219us       0.000us         0.00%       0.000us       0.000us          1000  \n",
      "                    aten::resize_         0.10%      80.345ms         0.10%      80.345ms       1.296us       0.000us         0.00%       0.000us       0.000us         62000  \n",
      "                      aten::empty         0.13%      99.055ms         0.13%      99.055ms       2.416us       0.000us         0.00%       0.000us       0.000us         41000  \n",
      "                       aten::view         0.16%     123.596ms         0.16%     123.596ms       5.374us       0.000us         0.00%       0.000us       0.000us         23000  \n",
      "                 aten::empty_like         0.02%      13.297ms         0.04%      28.810ms       7.203us       0.000us         0.00%       0.000us       0.000us          4000  \n",
      "                          aten::t         0.02%      13.850ms         0.04%      27.936ms       9.312us       0.000us         0.00%       0.000us       0.000us          3000  \n",
      "                  aten::transpose         0.01%       6.275ms         0.02%      14.086ms       4.695us       0.000us         0.00%       0.000us       0.000us          3000  \n",
      "                     aten::expand         0.01%       6.157ms         0.01%       8.809ms       2.936us       0.000us         0.00%       0.000us       0.000us          3000  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 78.103s\n",
      "Self CUDA time total: 79.312s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# profile pytorch by cuda time total\n",
    "print(pytorch_profiler.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddde7b6",
   "metadata": {},
   "source": [
    "Loading model to gpu seems to be the main problem of latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdff73",
   "metadata": {},
   "source": [
    "Export pytorch to onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9dd2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx version: 1.12.0\n",
      "onnxruntime version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort.set_default_logger_severity(0)\n",
    "\n",
    "print(\"onnx version:\", onnx.__version__)\n",
    "print(\"onnxruntime version:\", ort.__version__)\n",
    "\n",
    "onnx_version = onnx.__version__.split('.')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9d44f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'VGGReg' object has no attribute 'graph'\n"
     ]
    }
   ],
   "source": [
    "# input example necessary to export onnx model\n",
    "torch_input = torch.randn(batch_size, 3, 512, 512, requires_grad=True).to(device)\n",
    "\n",
    "model = Estimator((512, 512), model_path).model\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, # model being run\n",
    "    torch_input, # model input (or a tuple for multiple inputs)\n",
    "    \"density.onnx\", # where to save the model (can be a file or file-like object)\n",
    "    opset_version=12,\n",
    "    export_params=True, # store the trained parameter weights inside the model file\n",
    "    input_names=['input'], # the model's input names\n",
    "    output_names=['output'], # the model's output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}) # variable length axes\n",
    "\n",
    "try:\n",
    "    # print human readable representation of the graph if exist\n",
    "    print(onnx.helper.printable_graph(model.graph))\n",
    "except AttributeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e350a1e",
   "metadata": {},
   "source": [
    "Before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX’s API. \n",
    "\n",
    "1. First, onnx.load(\"superres.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). \n",
    "2. Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ba4cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export is valid.\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"density.onnx\")\n",
    "# check consistency of the model. \n",
    "# if model is larger than 2GB, model should be checked with path instead of model itself\n",
    "print(\"ONNX export is valid.\") if onnx.checker.check_model(onnx_model) == None else print(\"ONNX export is invalid.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e7510",
   "metadata": {},
   "source": [
    "Compute inference output using onnx runtime.\n",
    "\n",
    "To run the model, it is necessary to create an inference session. Once it is created, the model is evaluated using `run()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33ae2b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX and PyTorch results match!\n"
     ]
    }
   ],
   "source": [
    "ort.set_default_logger_severity(2)\n",
    "\n",
    "# create a onnx runtime session\n",
    "session = ort.InferenceSession(\"density.onnx\")\n",
    "\n",
    "# NOTE: This can be a bottleneck for gpu devices, since the tensor is transferred to cpu to\n",
    "# convert to numpy array. Then, the numpy array is transferred to gpu .\n",
    "\n",
    "# turn tensor into numpy array \n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# inference onnx\n",
    "onnx_output = session.run(None, {\"input\": to_numpy(torch_input)},)\n",
    "\n",
    "# inference pytorch\n",
    "torch_output = predictor.estimate(torch_input)\n",
    "\n",
    "# compare onnx X with pytorch results\n",
    "try:\n",
    "    np.testing.assert_allclose(to_numpy(torch_output), onnx_output[0], rtol=1e-03, atol=1e-05)\n",
    "    print(\"ONNX and PyTorch results match!\")\n",
    "except AssertionError as error:\n",
    "    print(\"ONNX and PyTorch results do not match!\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16403c87",
   "metadata": {},
   "source": [
    "Test latency inference output with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1b0e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as onnx_profiler:\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        onnx_input = {\"input\": to_numpy(torch_inputs[i:i+batch_size])}\n",
    "        onnx_output = session.run(None, onnx_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02144792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            aten::to        30.89%       9.258ms        30.89%       9.258ms       9.258us       4.676ms       100.00%       4.676ms       4.676us          1000  \n",
      "         aten::slice        52.08%      15.609ms        69.11%      20.713ms      20.713us       0.000us         0.00%       0.000us       0.000us          1000  \n",
      "    aten::as_strided        17.03%       5.104ms        17.03%       5.104ms       5.104us       0.000us         0.00%       0.000us       0.000us          1000  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 29.970ms\n",
      "Self CUDA time total: 4.676ms\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(onnx_profiler.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70f221",
   "metadata": {},
   "source": [
    "pytorch profiling doesn’t work for onnx model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('utils')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e890fa95ed0ec9c8fb286e04549f6077243facc59f1dcdc56b3adff9cce08d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
